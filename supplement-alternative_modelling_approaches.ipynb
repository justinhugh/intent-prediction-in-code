{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplemental Submission - Alternative Modelling Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook includes samples of the work done in applying a variety of tokenization and modelling approaches to the data set in order to create a method of predicting code intent. \n",
    "\n",
    "This is not an exhaustive account of the work done with alternative approaches. \n",
    "\n",
    "The cells below are not edited for report submission and are included to give a sample of the alternative approaches. Please keep in mind that the reporting and code below are incomplete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Patterns with Bag of Words Vectorization\n",
    "We can look at the above graph to see some common themes which emerge, at least on the level of word frequency. \n",
    "\n",
    "- String manipulation \n",
    "- List manipulation \n",
    "- Type change\n",
    "- Regular Expression\n",
    "- DataFrame Manipulation\n",
    "- Find object  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing `conala_train_df` with Bag of Words\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intent               0\n",
       "rewritten_intent    79\n",
       "snippet              0\n",
       "question_id          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for nan\n",
    "conala_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with \"\"\n",
    "conala_train_df.fillna('', inplace=True)\n",
    "\n",
    "conala_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate \n",
    "conala_train_bagofwords = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "\n",
    "# Fit \n",
    "conala_train_bagofwords.fit(conala_train_df[\"rewritten_intent\"])\n",
    "\n",
    "# Transform with the bag of words.\n",
    "conala_train_bag_SM = conala_train_bagofwords.transform(conala_train_df[\"rewritten_intent\"])\n",
    "conala_train_bag_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame (more workable) from the Sparse Matrix \n",
    "conala_train_bag_df = pd.DataFrame(columns=conala_train_bagofwords.get_feature_names(),\n",
    "                                   data=conala_train_bag_SM.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_train_bag_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing `conala_test_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nan\n",
    "conala_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with \"\"\n",
    "conala_test_df.fillna('', inplace=True)\n",
    "\n",
    "conala_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform with the bag of words from the train df\n",
    "conala_test_bag_SM = conala_train_bagofwords.transform(conala_test_df[\"rewritten_intent\"])\n",
    "conala_test_bag_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame (more workable) from the Sparse Matrix \n",
    "conala_test_bag_df = pd.DataFrame(columns=conala_train_bagofwords.get_feature_names(),\n",
    "                                   data=conala_test_bag_SM.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is our test set, we shouldn't peek at the results of the transformation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction of Bag of Words\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA on Bag of Words\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### T-SNE on Bag of Words\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Text Vectorization\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "Word2Vec Embeddings are \n",
    "\n",
    "See also Doc2Vec, FastText and wrappers for VarEmbed and WordRank.\n",
    "[[x]](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gensim, and get word2vec model methods. \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader # allows downloading of existing models\n",
    "\n",
    "# Downloading a pre-trained vector using 50 dimensions, from twitter data\n",
    "wv = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking vocab type\n",
    "type(wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms in vocab\n",
    "len(wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for similar terms, cosine similarity!\n",
    "wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if word is in wv vocab\n",
    "\"cat\" in wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique word are in our corpus?\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now check how many of these are in the word2vec pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the list of words contained in model, and those missing.\n",
    "contained=[] # list of terms in both our corpus and the model\n",
    "missing=[] # list of terms in our corpus, but not the model\n",
    "msk=[] # True/false mask for unique words that are in the model. \n",
    "for i in unique_words:\n",
    "    if(i in wv.vocab):\n",
    "        msk.append(1)\n",
    "        contained.append(i)\n",
    "    else:\n",
    "        msk.append(0)\n",
    "        missing.append(i)\n",
    "sum(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at missing words\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pre-existing vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Word2Vec, there's much extra thought to be given regarding how the sentences I'm feeding to the model will be handled. There's a large number of special characters such as brackets and \"%\" for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " comparing the unique words to vocab of pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple of functions to help process lists of text sentences.\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_split_text_list(li):\n",
    "    '''\n",
    "    Takes a list of sentences.\n",
    "    Returns a list of lists, each inner list is words in a sentence.\n",
    "    Also adds a space on either side of non-word, non-digit chars. \n",
    "    This allows for brackets, etc. to be considered as their own word, unless \n",
    "    vectorized with a model which does not include them.\n",
    "    '''\n",
    "    \n",
    "    new_list = list()\n",
    "    for i in li:\n",
    "        try:\n",
    "            i = i.lower() #lowercase the sentence\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            i = re.sub('([^a-zA-Z\\ \\d])', r' \\1 ', i) # Add spaces between special chars\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            i = list(i.split(' '))\n",
    "        except:\n",
    "            pass\n",
    "        new_list.append(i)\n",
    "    return new_list\n",
    "\n",
    "def vectorize_text_list(li):\n",
    "    '''\n",
    "    Takes a list of lists.\n",
    "        - first list is a sentence\n",
    "        - inner list is a list of words.\n",
    "    Returns a list of lists, each inner list is words in a sentence.\n",
    "    Also adds a space on either side of non-word, non-digit chars. \n",
    "    This allows for brackets, etc. to be considered as their own word, unless \n",
    "    vectorized with a model which does not include them.\n",
    "    '''\n",
    "    new_list=list() # new list object to be returned at end.\n",
    "    for i in li:\n",
    "        if i == None:\n",
    "            new_list.append(np.zeros_like(wv[\"empty\"])) # If None, empty array of wv shape.\n",
    "            continue\n",
    "        if type(i) == float:\n",
    "            i = str(i)\n",
    "        sub_list=list() # list of vecs, representing a sentence\n",
    "        for j in i: \n",
    "            try:\n",
    "                vec = wv[j]\n",
    "                sub_list.append(vec)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        new_list.append(sub_list)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Clustering Models to Find Intent Paradigms\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "One possible method of predicting intent would be to find clusters of similar code and intent fields. From these clusters, we can create a supervised learning classifier which attempts to predict the cluster that the code belongs to. \n",
    "\n",
    "We can look at the similarities in intent in these clusters and find patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this preliminary modelling, we'll work with: \n",
    "combined_bag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data, our goal is to identify a number of clusters which are \"similar\" to one another. These can give an understanding of the paradigms which are commonly found in code snippets (at least in Stack Overflow). \n",
    "\n",
    "So the plan of action will be to apply various clustering models to the vectorized data to see what we can learn from each in turn. The 4 we will try are: \n",
    "- Agglomerative\n",
    "- DB Scan\n",
    "- KMeans\n",
    "- Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "- Single\n",
    "- Maximum\n",
    "- Average\n",
    "- Ward's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioY5cN6-dADy",
    "outputId": "bc48a87e-5a0e-4210-c1bc-cf428a2d908e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# we are using the average linkage here\n",
    "linkagemat = linkage(combined_bag_df, 'average') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioY5cN6-dADy",
    "outputId": "bc48a87e-5a0e-4210-c1bc-cf428a2d908e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure(figsize=(25, 10))\n",
    "dendrogram(\n",
    "    linkagemat,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.  # font size for the x axis labels\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dendrogram above, we can see how the number of clusters reduces as the avereage distrance is increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vUuDq-zdAD6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "agg_clus = AgglomerativeClustering(n_clusters=20, linkage='average').fit(combined_bag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeKgx3b3dAD8",
    "outputId": "9d233552-7972-4195-944f-74bf1f53b817"
   },
   "outputs": [],
   "source": [
    "agg_clus.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(agg_clus.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "silhouette_score(combined_bag_df, agg_clus.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem all that helpful. We do have multiple clusters, but the vast majority of them lie in one.\n",
    "\n",
    "We can try to standard scale the data and run the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Fit \n",
    "ss_fit = ss.fit(combined_bag_df)\n",
    "\n",
    "# Transform\n",
    "combined_bag_df_ss = ss.transform(combined_bag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioY5cN6-dADy",
    "outputId": "bc48a87e-5a0e-4210-c1bc-cf428a2d908e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# we are using the average linkage here\n",
    "linkagemat = linkage(combined_bag_df_ss, 'average') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioY5cN6-dADy",
    "outputId": "bc48a87e-5a0e-4210-c1bc-cf428a2d908e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure(figsize=(25, 10))\n",
    "dendrogram(\n",
    "    linkagemat,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.  # font size for the x axis labels\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dendrogram above, we can see how the number of clusters reduces as the avereage distrance is increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vUuDq-zdAD6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "agg_clus = AgglomerativeClustering(n_clusters=20, linkage='average').fit(combined_bag_df_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model for rapid use later. \n",
    "agglom_model = open('pickled_agglom_model', 'ab+') \n",
    "\n",
    "# source, destination \n",
    "pickle.dump(agg_clus, agglom_model)                      \n",
    "agglom_model.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeKgx3b3dAD8",
    "outputId": "9d233552-7972-4195-944f-74bf1f53b817"
   },
   "outputs": [],
   "source": [
    "agg_clus.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(agg_clus.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "silhouette_score(combined_bag_df, agg_clus.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just as bad, and the silhouette score is worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "[[Back To TOC]](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "db = DBSCAN(eps=2, min_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.fit(combined_bag_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AQY0TpkdAEG",
    "outputId": "f6fa4b26-c77b-4733-a89a-af3e61de399d"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Instantiate\n",
    "db = DBSCAN(eps=2, min_samples=10)\n",
    "\n",
    "# Fit\n",
    "db.fit(combined_bag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AQY0TpkdAEG",
    "outputId": "f6fa4b26-c77b-4733-a89a-af3e61de399d"
   },
   "outputs": [],
   "source": [
    "#try this out with a range of eps and min_samples\n",
    "print(db.labels_.sum()) # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(db.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not great results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a larger eps, reduce min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AQY0TpkdAEG",
    "outputId": "f6fa4b26-c77b-4733-a89a-af3e61de399d"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Instantiate\n",
    "db = DBSCAN(eps=4, min_samples=5)\n",
    "\n",
    "# Fit\n",
    "db.fit(combined_bag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3AQY0TpkdAEG",
    "outputId": "f6fa4b26-c77b-4733-a89a-af3e61de399d"
   },
   "outputs": [],
   "source": [
    "#try this out with a range of eps and min_samples\n",
    "print(db.labels_.sum()) # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(db.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_labelled_df = combined_bag_df.copy()\n",
    "db_labelled_df.insert(0,\"DB_label\", db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_zero = db_labelled_df[db_labelled_df[\"DB_label\"]==0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
