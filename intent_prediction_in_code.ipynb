{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Code Intent Prediction\n",
    "## With Applied Machine Learning Techniques\n",
    "***\n",
    "### Justin Hugh\n",
    "#### Data Science Diploma Candidate, BrainStation\n",
    "##### December 18, 2020\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "## [ Introduction](#Introduction)\n",
    "## [ Limitations and Assumptions](#Limitations-and-Assumptions)\n",
    "## [ Background](#Background)\n",
    "## [ The Data](#The-Data)  \n",
    "### [- Sources of Data](#Sources-of-Data)  \n",
    "### [- Data Characteristics](#Data-Characteristics)  \n",
    "## [ Data Wrangling](#Data-Wrangling)  \n",
    "## [ Conclusion](#Conlusion)  \n",
    "## [ References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[[Back to TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Software and code are becoming unavoidable in our daily lives both personal and professional, but only a fraction of us are literate in code. Even among developers, there exist a wide range of languages and frameworks so no one is familiar with it all. A model which could predict the intent of code would be hugely impactful for:  \n",
    "- Education. Making code more accessible and interpretable.  \n",
    "- Security. Identifying code with malicious intent.  \n",
    "- Development. Providing contextual tooltips, suggestions, resources.   \n",
    "\n",
    "The goal of this project is to develop an ML model employing NLP tools to interpret what a piece of code is trying to accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations and Assumptions\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll recognize some of the limitiations and assumptions to the modelling and analysis we will conduct. Those listed here will are generally applicable to the project at large. Any that are more specifically applicable to a certain step are discussed at that point in the analysis.\n",
    "\n",
    "- Some of this data is not current. One of the main sources of our data comes from a competition which was conducted in 2018. Software changes quite quickly since updates to packages are relatively cheaply accomplished. My model and this system's performance may be less applicable to presently constructed code, and will deprecate over time as libraries and languages are updated.\n",
    "\n",
    "- I assume this data set does not have known significant errors, such as incorrect application of code or erroneous syntax. If these are present in abundance, then this system's performance will have \"learned\" incorrect code application. \n",
    "\n",
    "- Developers are not uniquely identified in the data I've used. Not having this information restricts me from making more deep insights into code and intent on a developer-by-developer level which could potentially mean more accurate interpretations. However, this is a good and necessary practice from a privacy and standpoint. If developers were uniquely identified in the data, this could potentially be used to reconstruct personal data, constituting a notable privacy concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "In any project there's always important context other than the code and the model. In this section, we'll discuss the important subject matter surround the problems we're tackling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Libraries\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "There's a wealth of support openly available in the form of packages for Machine Learning, and other problem areas we'll touch on in this project. \n",
    "\n",
    "We'll import some necessary ones below in this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-55b711f7c652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# NLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# The usual packages\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "# Data Wrangling\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# The classifiers \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# NLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "A model is only as good as the data it uses. In this section we'll discuss the inputs in this project; where it comes from, what it looks like, and how we hope to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of Data\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "To acquire the data used in this project, I accessed numerous resources hoping to create a varied, but informative and value-added data set. \n",
    "\n",
    "### CoNaLa\n",
    "\n",
    "[_The Code/Natural Language Challenge (CoNaLa)_](https://conala-corpus.github.io/#dataset-information) is a challenge that was created by [_Carnegie Mellon University (CMU)_](https://www.cmu.edu/) along with [_NeuLab_](http://www.cs.cmu.edu/~neulab/) and [_STRUDEL Lab_](https://cmustrudel.github.io/) on May 31, 2018 in order to test systems for generating programs from natural language [[1]](#References). The original intent was to - given an english input such as \"sort list x in reverse order\" - have a system output `x.sort(reverse=True)` in Python. \n",
    "\n",
    "_CoNaLa_ is a competition with no end date, and are offered for use within the challenge itself, or any other research on the intersection of code and natural languague - which this project falls nicely into.\n",
    "\n",
    "_CoNaLa_ provides a wealth of publicly available data which is well suited for the needs of this project (and ours) including: \n",
    "- Data crawled from _Stack Overflow_ with 2,379 training examples, and 500 test examples. These have been curated by annotators.\n",
    "- Automatically-mined data with 600,000 examples. \n",
    "- Links to other helpful and similar data sets:\n",
    "    - [Django Dataset](https://ahcweb01.naist.jp/pseudogen/)  \n",
    "    - [StaQC](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset)  \n",
    "    - [Code Docstring Corpus](https://github.com/EdinburghNLP/code-docstring-corpus)  \n",
    "    \n",
    "&&&&\n",
    "I accessed these data in a couple different ways (direct download from CoNaLa, git, etc.)\n",
    "&&&&"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Characteristics\n",
    "[[Back To TOC]](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "The purpose of Exploratory Data Analysis (EDA) is to familiarize ourselves with the data, determine whether it has missing values or other deficiencies, clean the data so it may be analyzed, and peek at some of the more immediately evident relations of the data and parameters we're working with. By the end of these activities, we will have a cleaned set of data which is prepared for modelling and deeper analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "Our data come from a variety of different sources, each requiring a different workflow in order to bring into this workbook and analyze. In this section we'll outline our methods for doing this. And import the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoNaLa Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Paradigms\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "(not exclusive?)\n",
    "- String manipulation\n",
    "- List manipulation \n",
    "- Type change\n",
    "- Regular Expression\n",
    "- DataFrame Manipulation\n",
    "- Find object  \n",
    "\n",
    "\n",
    "&&...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "[[Back To TOC]](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "[[Back To TOC]](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] CoNaLa: The Code/Natural Language Challenge. 2020. Conala: The Code/Natural Language Challenge. [online] Available at: <https://conala-corpus.github.io/#dataset-information> [Accessed 13 November 2020]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
