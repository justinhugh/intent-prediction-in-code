{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate dataframes to import.\n",
    "list_dfs = ['pickled_conala_mined_df', 'pickled_conala_train_df', 'pickled_conala_test_df',\n",
    "           'conala_train_bag_df', 'conala_mined_bag_df', 'combined_bag_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Load all data in list_dfs\n",
    "data = {}\n",
    "for df in list_dfs:\n",
    "    dbfile = open(df, 'rb')      \n",
    "    contents = pickle.load(dbfile)\n",
    "    data[df] = contents\n",
    "    dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pickled_conala_mined_df', 'pickled_conala_train_df', 'pickled_conala_test_df', 'conala_train_bag_df', 'conala_mined_bag_df', 'combined_bag_df'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['combined_bag_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Word2Vec, we need a list of all the sentences which will be transformed in it. So this will have to be done for both intent, and snippet. We can assemble this by combining the `conala_train_df` and the `conala_mined_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_train_df = data[\"pickled_conala_train_df\"]\n",
    "conala_mined_df = data[\"pickled_conala_mined_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two dfs.\n",
    "df = pd.concat([conala_train_df, conala_mined_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>rewritten_intent</th>\n",
       "      <th>snippet</th>\n",
       "      <th>question_id</th>\n",
       "      <th>parent_answer_post_id</th>\n",
       "      <th>prob</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to convert a list of multiple integers int...</td>\n",
       "      <td>Concatenate elements of a list 'x' of multiple...</td>\n",
       "      <td>sum(d * 10 ** i for i, d in enumerate(x[::-1]))</td>\n",
       "      <td>41067960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to convert a list of multiple integers int...</td>\n",
       "      <td>convert a list of integers into a single integer</td>\n",
       "      <td>r = int(''.join(map(str, x)))</td>\n",
       "      <td>41067960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how to convert a datetime string back to datet...</td>\n",
       "      <td>convert a DateTime string back to a DateTime o...</td>\n",
       "      <td>datetime.strptime('2010-11-13 10:33:54.227806'...</td>\n",
       "      <td>4170655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Averaging the values in a dictionary based on ...</td>\n",
       "      <td>get the average of a list values for each key ...</td>\n",
       "      <td>[(i, sum(j) / len(j)) for i, j in list(d.items...</td>\n",
       "      <td>29565452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zip lists in python</td>\n",
       "      <td>zip two lists `[1, 2]` and `[3, 4]` into a lis...</td>\n",
       "      <td>zip([1, 2], [3, 4])</td>\n",
       "      <td>13704860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5759</th>\n",
       "      <td>How to convert datetime to string in python in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{{(item.date | date): 'Y M d'}}</td>\n",
       "      <td>794995</td>\n",
       "      <td>795000.0</td>\n",
       "      <td>0.500243</td>\n",
       "      <td>794995_795000_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5760</th>\n",
       "      <td>Delete column from pandas DataFrame</td>\n",
       "      <td>NaN</td>\n",
       "      <td>df = df.drop('column_name', 1)</td>\n",
       "      <td>13411544</td>\n",
       "      <td>18145399.0</td>\n",
       "      <td>0.500193</td>\n",
       "      <td>13411544_18145399_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>How to get a list which is a value of a dictio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reverse_d = {value: key for key, values in lis...</td>\n",
       "      <td>40584186</td>\n",
       "      <td>40584271.0</td>\n",
       "      <td>0.500171</td>\n",
       "      <td>40584186_40584271_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>Cross-platform addressing of the config file</td>\n",
       "      <td>NaN</td>\n",
       "      <td>config_file = os.path.expanduser('~/foo.ini')</td>\n",
       "      <td>3227624</td>\n",
       "      <td>3227931.0</td>\n",
       "      <td>0.500164</td>\n",
       "      <td>3227624_3227931_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>how to change [1,2,3,4] to '1234' using python</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"\"\",\"\"\".join(['foo', 'bar', '', 'baz'])</td>\n",
       "      <td>2597932</td>\n",
       "      <td>2597937.0</td>\n",
       "      <td>0.500011</td>\n",
       "      <td>2597932_2597937_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5764 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 intent  \\\n",
       "0     How to convert a list of multiple integers int...   \n",
       "1     How to convert a list of multiple integers int...   \n",
       "2     how to convert a datetime string back to datet...   \n",
       "3     Averaging the values in a dictionary based on ...   \n",
       "4                                   zip lists in python   \n",
       "...                                                 ...   \n",
       "5759  How to convert datetime to string in python in...   \n",
       "5760                Delete column from pandas DataFrame   \n",
       "5761  How to get a list which is a value of a dictio...   \n",
       "5762       Cross-platform addressing of the config file   \n",
       "5763     how to change [1,2,3,4] to '1234' using python   \n",
       "\n",
       "                                       rewritten_intent  \\\n",
       "0     Concatenate elements of a list 'x' of multiple...   \n",
       "1      convert a list of integers into a single integer   \n",
       "2     convert a DateTime string back to a DateTime o...   \n",
       "3     get the average of a list values for each key ...   \n",
       "4     zip two lists `[1, 2]` and `[3, 4]` into a lis...   \n",
       "...                                                 ...   \n",
       "5759                                                NaN   \n",
       "5760                                                NaN   \n",
       "5761                                                NaN   \n",
       "5762                                                NaN   \n",
       "5763                                                NaN   \n",
       "\n",
       "                                                snippet  question_id  \\\n",
       "0       sum(d * 10 ** i for i, d in enumerate(x[::-1]))     41067960   \n",
       "1                         r = int(''.join(map(str, x)))     41067960   \n",
       "2     datetime.strptime('2010-11-13 10:33:54.227806'...      4170655   \n",
       "3     [(i, sum(j) / len(j)) for i, j in list(d.items...     29565452   \n",
       "4                                   zip([1, 2], [3, 4])     13704860   \n",
       "...                                                 ...          ...   \n",
       "5759                    {{(item.date | date): 'Y M d'}}       794995   \n",
       "5760                     df = df.drop('column_name', 1)     13411544   \n",
       "5761  reverse_d = {value: key for key, values in lis...     40584186   \n",
       "5762      config_file = os.path.expanduser('~/foo.ini')      3227624   \n",
       "5763            \"\"\",\"\"\".join(['foo', 'bar', '', 'baz'])      2597932   \n",
       "\n",
       "      parent_answer_post_id      prob                   id  \n",
       "0                       NaN       NaN                  NaN  \n",
       "1                       NaN       NaN                  NaN  \n",
       "2                       NaN       NaN                  NaN  \n",
       "3                       NaN       NaN                  NaN  \n",
       "4                       NaN       NaN                  NaN  \n",
       "...                     ...       ...                  ...  \n",
       "5759               795000.0  0.500243      794995_795000_0  \n",
       "5760             18145399.0  0.500193  13411544_18145399_2  \n",
       "5761             40584271.0  0.500171  40584186_40584271_0  \n",
       "5762              3227931.0  0.500164    3227624_3227931_0  \n",
       "5763              2597937.0  0.500011    2597932_2597937_1  \n",
       "\n",
       "[5764 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the text in intent field. (Note this is NOT using the \n",
    "# rewritten intent in the training data.)\n",
    "intent_text = list(df[\"intent\"])\n",
    "\n",
    "# Create a list of the code snippets in the data. \n",
    "snippet_text = list(df[\"snippet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How to convert a list of multiple integers into a single integer?', 'How to convert a list of multiple integers into a single integer?', 'how to convert a datetime string back to datetime object?', 'Averaging the values in a dictionary based on the key', 'zip lists in python', 'Prepend the same string to all items in a list', 'regex for repeating words in a string in Python', 'Normalizing a pandas DataFrame by row', 'swap values in a tuple/list inside a list in python?', 'swap values in a tuple/list inside a list in python?']\n",
      "['sum(d * 10 ** i for i, d in enumerate(x[::-1]))', \"r = int(''.join(map(str, x)))\", \"datetime.strptime('2010-11-13 10:33:54.227806', '%Y-%m-%d %H:%M:%S.%f')\", '[(i, sum(j) / len(j)) for i, j in list(d.items())]', 'zip([1, 2], [3, 4])', \"['hello{0}'.format(i) for i in a]\", \"re.sub('(?<!\\\\\\\\S)((\\\\\\\\S+)(?:\\\\\\\\s+\\\\\\\\2))(?:\\\\\\\\s+\\\\\\\\2)+(?!\\\\\\\\S)', '\\\\\\\\1', s)\", 'df.div(df.sum(axis=1), axis=0)', 'map(lambda t: (t[1], t[0]), mylist)', '[(t[1], t[0]) for t in mylist]']\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "print(intent_text[:10])\n",
    "print(snippet_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get each unique word in the text, and for the code, each unique char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5764/5764 [00:00<00:00, 204870.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'float',\n",
       " 'compose',\n",
       " 'possible',\n",
       " 'entry',\n",
       " '',\n",
       " 'Horizontal',\n",
       " 'Iterate',\n",
       " 'Bulk',\n",
       " 'fixed',\n",
       " 'only?',\n",
       " 'variables',\n",
       " 'capital',\n",
       " 'dot',\n",
       " 'app?',\n",
       " 'gradients',\n",
       " '0s',\n",
       " 'Max',\n",
       " 'box?',\n",
       " 'tabs',\n",
       " 'Pillow',\n",
       " 'converting',\n",
       " 'cookie',\n",
       " 'based',\n",
       " 're-indexing',\n",
       " 'utf-string',\n",
       " 'Office',\n",
       " 'Time',\n",
       " 'Expat',\n",
       " 'xlwt?',\n",
       " 'go',\n",
       " 'Evaluating',\n",
       " 'listing',\n",
       " 'pylab.savefig()',\n",
       " 'str(dict)?',\n",
       " 'figure?',\n",
       " 'psycopg2:',\n",
       " '0-dimension',\n",
       " 'gracefully',\n",
       " 'y-axis',\n",
       " 'concise',\n",
       " 'Contained',\n",
       " 'place',\n",
       " 'index?',\n",
       " 'parse',\n",
       " 'configure',\n",
       " 'App',\n",
       " 'modules',\n",
       " 'sub-level',\n",
       " 'python,',\n",
       " 'libraries?',\n",
       " 'unicode_literals',\n",
       " 'fashion',\n",
       " 'occurences',\n",
       " 'comma-separated',\n",
       " '(float)',\n",
       " 'correspond',\n",
       " 'value)',\n",
       " 'groupby:',\n",
       " 'Flask-Mail',\n",
       " 'them',\n",
       " 'doubling',\n",
       " 'integrate',\n",
       " 'bower',\n",
       " 'year?',\n",
       " 'pipe?',\n",
       " 'interactive?',\n",
       " 'Uniqueness',\n",
       " 'id?',\n",
       " 'crop',\n",
       " 'sets',\n",
       " 'pandas',\n",
       " 'driver',\n",
       " \"'£'\",\n",
       " 'SQLAlchemy-flask',\n",
       " 'Shell',\n",
       " 'Encoding',\n",
       " 'Pyhon',\n",
       " 'extension?',\n",
       " 'INSERT',\n",
       " 'visible?',\n",
       " 'repeating',\n",
       " 'slow',\n",
       " 'installed',\n",
       " 'subparsers',\n",
       " 'which',\n",
       " 'then',\n",
       " \"'classes'\",\n",
       " 'Python/Matplotlib',\n",
       " 'Cannot',\n",
       " '(now',\n",
       " 'commandline',\n",
       " '2]',\n",
       " 'IE',\n",
       " 'Django/Celery',\n",
       " 'PIL?',\n",
       " 'keywords',\n",
       " 'to/from',\n",
       " 'unpacking',\n",
       " 'process?',\n",
       " 'tab-delimited',\n",
       " 'curve',\n",
       " 'Terminating',\n",
       " 'even',\n",
       " 'union',\n",
       " 'zope',\n",
       " 'unreachable',\n",
       " 'rearrange',\n",
       " 'extractor',\n",
       " 'Separate',\n",
       " 'converted',\n",
       " 'Database',\n",
       " 'pythonic',\n",
       " 'Play',\n",
       " 'words/phrases',\n",
       " 'dimensional',\n",
       " 'continuous',\n",
       " 'floating',\n",
       " \"'maximized'\",\n",
       " 'PyQt',\n",
       " 'representations',\n",
       " \"pack('H')\",\n",
       " 'Construct',\n",
       " 'add',\n",
       " 'Why',\n",
       " 'positions',\n",
       " '(key',\n",
       " 'issue',\n",
       " 'argparse:',\n",
       " 'selection',\n",
       " 'Upload',\n",
       " 'truncation',\n",
       " 'Right',\n",
       " 'polygon',\n",
       " 'applications?',\n",
       " 'Object',\n",
       " 'primary',\n",
       " '(django)',\n",
       " '\"Only',\n",
       " '(elegantly)',\n",
       " 'Union',\n",
       " 'designate',\n",
       " '\"forward-fill\"',\n",
       " '(caveat)?',\n",
       " 'links',\n",
       " 'drawn',\n",
       " 'colours',\n",
       " 'currency',\n",
       " 'HTMLUnit',\n",
       " 'RequestContext',\n",
       " 'non-unique',\n",
       " 'Bold',\n",
       " 'nparray:',\n",
       " 'One',\n",
       " 'zip',\n",
       " 'accumulator',\n",
       " 'Document',\n",
       " 'sqlalchemy',\n",
       " 'do',\n",
       " 'subplot',\n",
       " 'representing',\n",
       " 'relationship',\n",
       " 'works',\n",
       " 'INT/CHAR',\n",
       " 'Changing',\n",
       " 'but',\n",
       " 'transfer',\n",
       " 'Delimiters?',\n",
       " 'Percentage',\n",
       " 'occur',\n",
       " 'barchar',\n",
       " 'chart',\n",
       " 'insensitive',\n",
       " 'vectorize',\n",
       " 'Item',\n",
       " 'closer',\n",
       " 'desired',\n",
       " 'UnicodeDecodeError',\n",
       " 'onto',\n",
       " 'Mask',\n",
       " 'PIL:',\n",
       " 'Get',\n",
       " 'Scrolling',\n",
       " 'tabular',\n",
       " 'from',\n",
       " 'data',\n",
       " 'PHP',\n",
       " 'concatenation,',\n",
       " \"'lol'\",\n",
       " 'datetime.datetime',\n",
       " 'filtering',\n",
       " 'unix',\n",
       " 'send',\n",
       " 'Framework',\n",
       " 'Inserting',\n",
       " 'likes',\n",
       " 'tools',\n",
       " 'printing',\n",
       " 'Pandas',\n",
       " 'logoff,',\n",
       " 'takes',\n",
       " 'matlab',\n",
       " 'gaps',\n",
       " 'multi-index?',\n",
       " 'itertools.groupby()',\n",
       " 'mm/dd/yyyy',\n",
       " 'un-escape',\n",
       " 'succinct',\n",
       " 'thousands',\n",
       " 'decoding',\n",
       " 'indexes,',\n",
       " 'Check',\n",
       " 'retrieving',\n",
       " 'non-ascii',\n",
       " 'Point',\n",
       " 'strings',\n",
       " 'As',\n",
       " 'weekdays',\n",
       " 'Assign',\n",
       " 'microtime',\n",
       " '(column,',\n",
       " 'database?',\n",
       " '(Python',\n",
       " 'following',\n",
       " '[2,',\n",
       " 'Searching',\n",
       " 'ElementTree?',\n",
       " 'summarizing',\n",
       " 'connect',\n",
       " 'image?',\n",
       " 'excluding',\n",
       " 'pymongo',\n",
       " 'Maximum',\n",
       " 'networkX',\n",
       " 'set_index',\n",
       " 'conversion',\n",
       " 'padded',\n",
       " '(dot)?',\n",
       " 'cleaner',\n",
       " 'executing',\n",
       " 'Columns',\n",
       " 'service?',\n",
       " 'valid',\n",
       " 'key/value',\n",
       " 'it?',\n",
       " 'reason',\n",
       " 'addition',\n",
       " 'offset',\n",
       " '-',\n",
       " 'condition',\n",
       " 'Legend?',\n",
       " 'replacements',\n",
       " 'subdirectories',\n",
       " 'root',\n",
       " 'resetting',\n",
       " 'line?',\n",
       " 'compare',\n",
       " 'less',\n",
       " 'authority',\n",
       " 'extending',\n",
       " '.py',\n",
       " 'Fullscreen',\n",
       " 'masks',\n",
       " 'Scikit-learn:',\n",
       " 'Engine',\n",
       " ')',\n",
       " 'varchar/string',\n",
       " 'Concatenating',\n",
       " 'multiline',\n",
       " 'data)',\n",
       " 'glob',\n",
       " 'stdout?',\n",
       " 'Group',\n",
       " 'If',\n",
       " 'Close',\n",
       " 'base64',\n",
       " 'consonants',\n",
       " 'Substring',\n",
       " 'dataframe',\n",
       " 'structure',\n",
       " 'easy',\n",
       " 'cropped',\n",
       " 'representation?',\n",
       " 'groupby-apply',\n",
       " 'dates?',\n",
       " 'Dealing',\n",
       " 'selenium/python/ubuntu',\n",
       " '[1,',\n",
       " 'print/show',\n",
       " 'SSH?',\n",
       " 'discontinuous',\n",
       " 'beautifulsoup',\n",
       " 'weeknumber',\n",
       " 'xlim',\n",
       " 'frequency?',\n",
       " 'comparing',\n",
       " 'Relevance',\n",
       " 'doesnt',\n",
       " 'lib',\n",
       " 'word',\n",
       " 'use',\n",
       " 'summing',\n",
       " 'In',\n",
       " 'Retrieving',\n",
       " 'byte',\n",
       " 'utc',\n",
       " 'separating',\n",
       " 'sqlite3',\n",
       " 'many-to-many',\n",
       " 'external',\n",
       " 'Deleting',\n",
       " 'x-axis',\n",
       " 'dicts,',\n",
       " 'wrapped',\n",
       " 'rows',\n",
       " '100.0?',\n",
       " 'abbreviation',\n",
       " 'sizes',\n",
       " 'fallback',\n",
       " 'timeseries',\n",
       " '|',\n",
       " 'pattern',\n",
       " 'only)',\n",
       " 'Updating',\n",
       " 'MultipartPostHandler',\n",
       " 'boxplot',\n",
       " 'merging',\n",
       " 'GET/POST',\n",
       " 'ordered',\n",
       " 'Dimensional',\n",
       " 'subprocess',\n",
       " 'Viewing',\n",
       " 'fourth',\n",
       " 'advice',\n",
       " 'positional',\n",
       " 'zero?',\n",
       " 'Studio',\n",
       " 'md5',\n",
       " 'DataFrame,',\n",
       " 'cloud',\n",
       " 'Pylons?',\n",
       " 'arrays?',\n",
       " 'href',\n",
       " 'input',\n",
       " 'quotes',\n",
       " 'operator',\n",
       " 'Numpy',\n",
       " 'SCP',\n",
       " 'three',\n",
       " 'dictionary/list',\n",
       " '`[:space:]`?',\n",
       " 'subproccess?',\n",
       " 'complex',\n",
       " 'list',\n",
       " '(intersection)',\n",
       " 'child',\n",
       " 'minimum',\n",
       " 'sum',\n",
       " 'entities',\n",
       " 'cast',\n",
       " 'regexp-like',\n",
       " '\"Downloads\"',\n",
       " 'decimal?',\n",
       " 'dics',\n",
       " 'x11',\n",
       " 'ordering',\n",
       " 'Sort',\n",
       " 'quantiles',\n",
       " 'Keyboard',\n",
       " 'FB',\n",
       " 'GPS',\n",
       " \"what's\",\n",
       " 'Scipy',\n",
       " 'y',\n",
       " 'draw',\n",
       " 'replacing',\n",
       " 'figure',\n",
       " 'GET',\n",
       " 'selenium',\n",
       " '2.0',\n",
       " 'Named',\n",
       " 'octal',\n",
       " 'errorbars',\n",
       " 'post',\n",
       " 'PID',\n",
       " 'dictionary',\n",
       " 'transition',\n",
       " 'frequent',\n",
       " \"row's\",\n",
       " 'between',\n",
       " 'matrix?',\n",
       " 'bash',\n",
       " 'pages',\n",
       " 'newline',\n",
       " 'euclidean',\n",
       " 'space',\n",
       " 'else',\n",
       " 'Parentheses',\n",
       " 'special',\n",
       " 'Django:',\n",
       " 'another',\n",
       " 'python',\n",
       " 'Multiindex',\n",
       " 'Find',\n",
       " 'according',\n",
       " 'Subtract',\n",
       " 'Non-consuming',\n",
       " 'ISO-8859-1/latin1',\n",
       " 'Function',\n",
       " 'Where',\n",
       " '100',\n",
       " 'pymongo:',\n",
       " 'for',\n",
       " 'xpath?',\n",
       " 'DOS',\n",
       " 'wrap_socket',\n",
       " 'Multipart',\n",
       " 'Build',\n",
       " 'transparent',\n",
       " 'MITM',\n",
       " 'prompt',\n",
       " 'REST',\n",
       " 'Delete',\n",
       " 'prompting',\n",
       " 'scratch',\n",
       " 'FigureCanvas',\n",
       " 'emoticons',\n",
       " 'Replacing',\n",
       " '3',\n",
       " 'Panda',\n",
       " 'urlencode',\n",
       " '\"actual',\n",
       " 'entries',\n",
       " 'Show',\n",
       " '(python)',\n",
       " 'smaller',\n",
       " 'const',\n",
       " 'Generator',\n",
       " 'hide',\n",
       " 'empty',\n",
       " 'searching',\n",
       " 'TimeSeries?',\n",
       " '(including',\n",
       " 'vector',\n",
       " 'interface',\n",
       " 'white',\n",
       " 'Jinja2',\n",
       " 'SymPy?',\n",
       " \"'tag'\",\n",
       " 'Remove',\n",
       " 'ObjC?',\n",
       " 'halt',\n",
       " 'Ctrl',\n",
       " 'Execute',\n",
       " 'Do',\n",
       " 'mid-graph',\n",
       " 'containing',\n",
       " 'datetime',\n",
       " 'gui',\n",
       " 'format',\n",
       " 'self',\n",
       " '2.7)',\n",
       " 'password?',\n",
       " \"doesn't\",\n",
       " 'Concat',\n",
       " 'DELETE',\n",
       " 'distinct',\n",
       " 'class?',\n",
       " 'without',\n",
       " 'looking',\n",
       " '__init__',\n",
       " 'duration',\n",
       " 'testing?',\n",
       " 'key,',\n",
       " 'Norm',\n",
       " 'through',\n",
       " '-u',\n",
       " 'Move',\n",
       " 'consecutive',\n",
       " 'Elegant',\n",
       " 'Symlinks',\n",
       " 'fold/accumulate',\n",
       " 'NumPy-thonic?',\n",
       " 'occurrences',\n",
       " 'Rest',\n",
       " 'markdown',\n",
       " 'variable',\n",
       " 'update',\n",
       " 'create',\n",
       " 'is',\n",
       " 'Rearrange',\n",
       " 'bar?',\n",
       " \"array's\",\n",
       " 'Resampling',\n",
       " 'comparison',\n",
       " 'faster',\n",
       " 'raw_input()',\n",
       " 'easiest',\n",
       " 'way',\n",
       " 'rid',\n",
       " 'border',\n",
       " 'directory?',\n",
       " 'texts?',\n",
       " 'Fetch',\n",
       " 'similarity',\n",
       " 'Parameters',\n",
       " 'deleting',\n",
       " 'gtk.Window?',\n",
       " 'events',\n",
       " \"'if's\",\n",
       " 'selector',\n",
       " '(Network',\n",
       " 'parameters?',\n",
       " 'Slicing',\n",
       " 'interactive',\n",
       " \"'last\",\n",
       " 'export',\n",
       " 'result',\n",
       " 'reading',\n",
       " 'Manipulating',\n",
       " 'UrlFetch',\n",
       " 'read_csv',\n",
       " 'unseen',\n",
       " 'contiguous',\n",
       " 'substring',\n",
       " 'each',\n",
       " 'fill',\n",
       " 'Unescaping',\n",
       " 'Speed',\n",
       " 'call',\n",
       " 'Parent',\n",
       " 'Configuration',\n",
       " 'hangs',\n",
       " \"Class's\",\n",
       " 'working',\n",
       " 'explosion',\n",
       " 'Sphinx',\n",
       " 'Match',\n",
       " '2.7?',\n",
       " 'region',\n",
       " 'added',\n",
       " 'decmial',\n",
       " 'plus',\n",
       " 'together',\n",
       " 'since',\n",
       " 'literal?',\n",
       " 'Plot?',\n",
       " 'inset)',\n",
       " 'session',\n",
       " 'business',\n",
       " 'Counter',\n",
       " 'dataframes?',\n",
       " 'sprintf',\n",
       " 'reader',\n",
       " 'hatch',\n",
       " '(Windows',\n",
       " 'years',\n",
       " 'document',\n",
       " 'whether',\n",
       " 'starting',\n",
       " 'MYSQLdb.',\n",
       " 'python-numpy:',\n",
       " 'regex',\n",
       " 'store',\n",
       " 'while',\n",
       " 'The',\n",
       " 'used?',\n",
       " 'Ordered',\n",
       " 'row)',\n",
       " 'ternary',\n",
       " 'axis',\n",
       " 'space?',\n",
       " 'savepoints',\n",
       " 'alpha',\n",
       " 'files?',\n",
       " 'fromfunction',\n",
       " 'escaping',\n",
       " '98]',\n",
       " 'null',\n",
       " \"'1'\",\n",
       " 'pyqt',\n",
       " 'power',\n",
       " 'Strings',\n",
       " 'expressions?',\n",
       " 'Redis:',\n",
       " 'NodeJS',\n",
       " 'ratings',\n",
       " 'weird',\n",
       " 'markers',\n",
       " 'see',\n",
       " 'proxy',\n",
       " 'statement',\n",
       " 'FileField?',\n",
       " 'levels',\n",
       " 'Duplicating',\n",
       " 'items',\n",
       " 'pad',\n",
       " 'flask',\n",
       " 'MySQL',\n",
       " 'Filtering',\n",
       " 'build',\n",
       " 'notepad',\n",
       " 'perpendicular',\n",
       " 'df',\n",
       " 'latin1',\n",
       " 'CPython',\n",
       " 'numpy.array?',\n",
       " 'unit',\n",
       " 'redis?',\n",
       " 'Cleanest',\n",
       " 'curses',\n",
       " 'interpreter?',\n",
       " 'time?',\n",
       " 'Python(pandas):',\n",
       " 'take',\n",
       " 'grab',\n",
       " 'one-dimensional',\n",
       " '(in',\n",
       " 'arrow',\n",
       " 'AttributeError',\n",
       " 'XML',\n",
       " 'numbers?',\n",
       " 'attributes?',\n",
       " 'proper',\n",
       " 'byte\"',\n",
       " 'bytearray',\n",
       " 'keys',\n",
       " '3000-line',\n",
       " 'map',\n",
       " 'months',\n",
       " 'break',\n",
       " 'ranges',\n",
       " 'operations',\n",
       " 'utf',\n",
       " 'occurence',\n",
       " 'raising',\n",
       " 'Missing',\n",
       " 'Fails',\n",
       " 'array...if',\n",
       " 'figures',\n",
       " 'website',\n",
       " 'length?',\n",
       " 'numpy.random.choice',\n",
       " 'url_for',\n",
       " 'Matplotlib:',\n",
       " 'App?',\n",
       " 'representation',\n",
       " 'than',\n",
       " 'under',\n",
       " 'handling',\n",
       " 'cut',\n",
       " 'solve',\n",
       " 'Importing',\n",
       " 'game',\n",
       " 'largest',\n",
       " 'non-numeric',\n",
       " ',',\n",
       " 'delta',\n",
       " 'polar',\n",
       " 'Paramiko?',\n",
       " '\"UnicodeDecodeError:',\n",
       " 'Moving',\n",
       " 'rule',\n",
       " '2D',\n",
       " 'to:',\n",
       " 'Possible',\n",
       " 'circumcentres',\n",
       " 'RAM',\n",
       " 'shell?',\n",
       " 'scatterplot',\n",
       " 'command-line?',\n",
       " 'Chart',\n",
       " 'app',\n",
       " \"can't\",\n",
       " 'strings)',\n",
       " '\"vector\"',\n",
       " 'Fit',\n",
       " 'Django/postgreSQL?',\n",
       " 'Modify',\n",
       " 'shapes',\n",
       " 'pyodbc',\n",
       " 'reopen',\n",
       " 'CRLF?',\n",
       " 'Conditionally',\n",
       " 'Is',\n",
       " 'Perl',\n",
       " 'TAB',\n",
       " 'AttributeError:',\n",
       " 'Script',\n",
       " 'agent',\n",
       " 'Problem',\n",
       " 'generating',\n",
       " 'apply',\n",
       " 'TensorFlow?',\n",
       " 'shortest',\n",
       " 'labels)',\n",
       " 'GROUP',\n",
       " 'directory(package)',\n",
       " 'appengine',\n",
       " 'grouping',\n",
       " 'json?',\n",
       " 'findall',\n",
       " 'remove',\n",
       " 'diagonal',\n",
       " 'across',\n",
       " 'options',\n",
       " 'lambda,',\n",
       " '(python)?',\n",
       " 'filter',\n",
       " 'iterables',\n",
       " 'C++',\n",
       " 'columns,',\n",
       " 'extraneous',\n",
       " 'lives',\n",
       " 'repeat',\n",
       " 'many',\n",
       " 'boolean',\n",
       " 'subscript',\n",
       " 'GAE',\n",
       " 'project?',\n",
       " 'equations',\n",
       " '(of',\n",
       " 'Ellipsoid',\n",
       " 'True',\n",
       " 'dynamically',\n",
       " 'tells',\n",
       " 'Superscript',\n",
       " 'inside',\n",
       " 'parameter',\n",
       " 'order?',\n",
       " 'larger',\n",
       " 'div',\n",
       " 'permutations',\n",
       " 'versa',\n",
       " 'ndarray?',\n",
       " 'mismatch',\n",
       " 'the',\n",
       " 'and',\n",
       " 'generate',\n",
       " 'dimensions',\n",
       " 'number?',\n",
       " 'Context',\n",
       " 'written',\n",
       " 'component',\n",
       " 'elementwise',\n",
       " 'colormap',\n",
       " 'websocket',\n",
       " 'to',\n",
       " 'date_parser',\n",
       " 'hysteresis',\n",
       " 'Sound',\n",
       " 'download',\n",
       " 'switching',\n",
       " 'status',\n",
       " 'A4',\n",
       " 'line',\n",
       " 'cookies',\n",
       " '[[5,',\n",
       " 'simplest',\n",
       " 'encoding',\n",
       " 'decryption',\n",
       " 'JSON',\n",
       " 'Capture',\n",
       " 'count',\n",
       " 'what',\n",
       " 'digits',\n",
       " 'rational',\n",
       " 'dynamic',\n",
       " 'Update',\n",
       " 'Base',\n",
       " 'Implementing',\n",
       " 'loop?',\n",
       " 'files',\n",
       " 'Combinatorial',\n",
       " 'numpy/scipy',\n",
       " 'refer',\n",
       " 'PNG',\n",
       " '*actually*',\n",
       " 'table',\n",
       " 'justified',\n",
       " 'Url',\n",
       " 'correctly',\n",
       " 'types',\n",
       " 'few',\n",
       " 'numpy.ndarray',\n",
       " 'mean',\n",
       " '(CentOS)',\n",
       " 'overwrite',\n",
       " 'anything',\n",
       " 'Matrix',\n",
       " 'flat',\n",
       " 'randomising',\n",
       " 'backslash',\n",
       " '3.2',\n",
       " 'clip',\n",
       " 'A',\n",
       " 'csr_matrix',\n",
       " 'missing',\n",
       " 'counting',\n",
       " '(matplotlib,',\n",
       " 'uniqify',\n",
       " 'extract',\n",
       " 'know',\n",
       " 'ArtistAnimation',\n",
       " 'Literal',\n",
       " 'Best',\n",
       " 'multiple',\n",
       " 'member-dict',\n",
       " 'importlib.import_module',\n",
       " 'BeautifulSoup',\n",
       " 'Outer',\n",
       " 'cumulative',\n",
       " 'Combination',\n",
       " 'filled',\n",
       " 'container',\n",
       " 'field',\n",
       " 'zipped',\n",
       " 'Sending',\n",
       " 'slash',\n",
       " 'party',\n",
       " 'simple/efficient',\n",
       " 'min',\n",
       " '*',\n",
       " 'webcam)',\n",
       " 'connection',\n",
       " 'manipulation',\n",
       " 'python-2.7',\n",
       " '^M',\n",
       " 'checksum',\n",
       " 'Pythonically',\n",
       " 'put',\n",
       " 'level?',\n",
       " 'lxml?',\n",
       " 'css',\n",
       " 'subprcess',\n",
       " 'GroupBy',\n",
       " 'their',\n",
       " 'determine',\n",
       " 'understand',\n",
       " 'extra',\n",
       " 'pipe',\n",
       " 'models?',\n",
       " '0]]',\n",
       " 'numpy:',\n",
       " 'widget',\n",
       " 'Access',\n",
       " 'symbolic',\n",
       " 'security',\n",
       " 'list,',\n",
       " 'testing',\n",
       " 'library',\n",
       " 'users',\n",
       " 'forward',\n",
       " 'syslog',\n",
       " 'dummies',\n",
       " 'non-greedy',\n",
       " 'depending',\n",
       " 'circles',\n",
       " 'Eclipse',\n",
       " 'Set',\n",
       " 'functions',\n",
       " 'write-iterators',\n",
       " 'Line',\n",
       " 'alphanumeric',\n",
       " 'embedded',\n",
       " 'does',\n",
       " 'Disable',\n",
       " 'quantifiers',\n",
       " 'Accessing',\n",
       " 'noreferrer\">ASCII</a>',\n",
       " 'aloud',\n",
       " 'dataseries',\n",
       " 'Python-',\n",
       " 'ints',\n",
       " 'whole',\n",
       " 'assign',\n",
       " 'jump',\n",
       " 'said',\n",
       " 'sub-lists',\n",
       " 'site',\n",
       " '530)',\n",
       " 'Creating',\n",
       " 'None?',\n",
       " 'here?',\n",
       " 'preceding',\n",
       " 'non-alphabet',\n",
       " 'TKinter',\n",
       " 'first',\n",
       " \"[Fix'd]\",\n",
       " 'int?',\n",
       " 'Expansion',\n",
       " 'FreeBSD',\n",
       " 'specify',\n",
       " 'Every',\n",
       " 'confusion',\n",
       " 'full(absolute',\n",
       " '3.6',\n",
       " 'reopen?',\n",
       " 'start',\n",
       " '(pandas)',\n",
       " 'static',\n",
       " 'lists?',\n",
       " 'dtype',\n",
       " 'literal',\n",
       " 'GhostDriver',\n",
       " 'strings:',\n",
       " '(python',\n",
       " 'pygame',\n",
       " 'circle',\n",
       " 'constraint?',\n",
       " 'marker',\n",
       " 'camera',\n",
       " 'stdin',\n",
       " 'belief',\n",
       " 'console',\n",
       " 'two-dimensional',\n",
       " 'Slashes',\n",
       " 'SSL',\n",
       " 'invalid',\n",
       " 'of',\n",
       " 'mail',\n",
       " 'MultiIndex?',\n",
       " '(sub-)',\n",
       " 'Dataframe?',\n",
       " 'Zipfile',\n",
       " 'printf',\n",
       " 'Obtaining',\n",
       " 'request',\n",
       " 'Case',\n",
       " 'email',\n",
       " 'chart?',\n",
       " 'Decimal',\n",
       " 'latex',\n",
       " 'cell',\n",
       " 'URL',\n",
       " 'Python',\n",
       " 'Line2D',\n",
       " 'nlargest()',\n",
       " 'Pyside',\n",
       " 'checked',\n",
       " 'PANDAS,',\n",
       " 'dropping',\n",
       " 'stdin?',\n",
       " 'key',\n",
       " 'so',\n",
       " 'Split',\n",
       " 'folders?',\n",
       " 'jpg',\n",
       " 'Blueprints,',\n",
       " 'if/else',\n",
       " 'slice',\n",
       " 'offset?',\n",
       " 'view',\n",
       " 'dictionary,',\n",
       " 'LIST',\n",
       " 'Splitting',\n",
       " 'matchings',\n",
       " 'numpy.recarray',\n",
       " 'declaring',\n",
       " 'type',\n",
       " 'list.',\n",
       " 'segments',\n",
       " 'simulating',\n",
       " 'Variable',\n",
       " 'triangulation',\n",
       " 'yerr',\n",
       " 'runs',\n",
       " 'ubuntu',\n",
       " 'fix:',\n",
       " 'POST,',\n",
       " 'Engine?',\n",
       " '\"divide\"',\n",
       " 'algorithm',\n",
       " 'CSV',\n",
       " 'axes',\n",
       " 'Complex',\n",
       " 'package',\n",
       " 'tags',\n",
       " 'ragged',\n",
       " 'mysql.connector',\n",
       " 'Filter',\n",
       " '.png',\n",
       " 'get',\n",
       " 'Selenium?',\n",
       " 'UTF-16',\n",
       " 'top',\n",
       " 'substitutions',\n",
       " 'foreign',\n",
       " 'pandas.core.series.TimeSeries',\n",
       " 'triple-loop',\n",
       " '\"unpivot\"',\n",
       " 'textarea',\n",
       " 'OneToOne',\n",
       " 'via',\n",
       " 'average?',\n",
       " 'rel=\"nofollow',\n",
       " 'Elements',\n",
       " 'usage',\n",
       " '(but',\n",
       " 'Map',\n",
       " 'Image',\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique words in text\n",
    "intent_tokens = set()\n",
    "    \n",
    "for intent in tqdm(intent_text):\n",
    "    for word in intent.split(\" \"):\n",
    "        intent_tokens.add(word)\n",
    "\n",
    "num_intent_tokens = len(intent_tokens)\n",
    "intent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5764"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3658"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_intent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5764/5764 [00:00<00:00, 27912.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('How', 'to'),\n",
       " ('How', 'convert'),\n",
       " ('How', 'a'),\n",
       " ('to', 'How'),\n",
       " ('to', 'convert'),\n",
       " ('to', 'a'),\n",
       " ('convert', 'How'),\n",
       " ('convert', 'to'),\n",
       " ('convert', 'a'),\n",
       " ('a', 'How'),\n",
       " ('a', 'to'),\n",
       " ('a', 'convert'),\n",
       " ('to', 'convert'),\n",
       " ('to', 'a'),\n",
       " ('to', 'list'),\n",
       " ('convert', 'to'),\n",
       " ('convert', 'a'),\n",
       " ('convert', 'list'),\n",
       " ('a', 'to'),\n",
       " ('a', 'convert')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the data with N-grams\n",
    "from nltk import ngrams\n",
    "import itertools\n",
    "\n",
    "gram_size = 4\n",
    "data = []\n",
    "\n",
    "# Go over each intent statement\n",
    "for intent in tqdm(intent_text):\n",
    "    # Finds all n-grams in the statement\n",
    "    grams = ngrams(intent.split(), gram_size)\n",
    "    for gram in grams:\n",
    "        # Find all pairs of words within this n-gram\n",
    "        for pair in itertools.permutations(gram, 2):\n",
    "            data.append(pair)\n",
    "\n",
    "data[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400836"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = encoder.fit(list(intent_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3658"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_hot_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 217.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#Transform the input/output pairs:\n",
    "intent_train_data = []\n",
    "intent_train_target = []\n",
    "\n",
    "for pair in tqdm(data[:1000]):\n",
    "    intent_train_data.append(one_hot_encoder.transform([pair[0]]))\n",
    "    intent_train_target.append(one_hot_encoder.transform([pair[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_train_data = np.squeeze(np.asarray(intent_train_data))\n",
    "intent_train_target = np.squeeze(np.asarray(intent_train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Pickle the data for use later, avoiding lengthy one-hot encoding again. \n",
    "# intent_train_data\n",
    "with open('pickled_intent_train_data.pkl', 'wb+') as f:\n",
    "    # source, destination \n",
    "    pickle.dump(intent_train_data, f)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3658)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8dca2ec2160b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cleaning the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocessed_intent_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintent_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Cleaning the text\n",
    "processed_intent_text = intent_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U21'), dtype('<U21')) -> dtype('<U21')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-7d9858e22431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintent_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             self.train(\n\u001b[1;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    927\u001b[0m             trim_rule=trim_rule, **kwargs)\n\u001b[1;32m    928\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, hs, negative, wv, update, vocabulary)\u001b[0m\n\u001b[1;32m   1685\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[0;34m(self, hs, negative, wv)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m             \u001b[0;31m# construct deterministic seed from word AND seed argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m             \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseeded_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U21'), dtype('<U21')) -> dtype('<U21')"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(intent_train_data, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent_train_target\n",
    "with open('pickled_intent_train_target.pkl', 'wb+') as f:\n",
    "    # source, destination \n",
    "    pickle.dump(intent_train_target, f)                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(keras.layers.Dense(num_intent_tokens, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.CategoricalCrossentropy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e7a84e61df50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Printout a single verbose fit operation 10 times throughout the training process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintent_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintent_train_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {(i+1)*round(num_epochs/10)}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/justin.hugh/opt/anaconda3/envs/deeplearning/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type csr_matrix)."
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "# Printout a single verbose fit operation 10 times throughout the training process.\n",
    "for i in range(0, 10):\n",
    "    model.fit(intent_train_data, intent_train_target, epochs=round(num_epochs/10)-1, verbose=0)\n",
    "    \n",
    "    print(f\"Epoch: {(i+1)*round(num_epochs/10)}/{num_epochs}\")\n",
    "    model.fit(intent_train_data, intent_train_target, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pcs = 200\n",
    "# Instantiate\n",
    "myPCA = PCA(n_components=200)\n",
    "# Fit\n",
    "myPCA.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_var = myPCA.explained_variance_ratio_\n",
    "expl_var_cumulative = myPCA.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(range(num_pcs), expl_var_cumulative, color='cornflowerblue')\n",
    "plt.title(\"Explained Variance by Number of PCs\")\n",
    "plt.xlabel(\"Number of PCs\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a diminishing return in the explained variance with respect to the number of PCs.\n",
    "But it's heartening that the explained variance begins with a steep increase in variance explained. \n",
    "Let's take the explained variance thresholde to be 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of PCs for 0.8 explained var.\n",
    "num_PCs = np.argmax(expl_var_cumulative > 0.8)\n",
    "num_PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refitting with 129 PCs\n",
    "\n",
    "# Instantiate\n",
    "myPCA = PCA(n_components=num_PCs)\n",
    "# Fit\n",
    "myPCA.fit(df)\n",
    "# Transform\n",
    "df_PC = myPCA.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vUuDq-zdAD6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg_clus = AgglomerativeClustering(n_clusters=3, linkage='average').fit(df_PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeKgx3b3dAD8",
    "outputId": "9d233552-7972-4195-944f-74bf1f53b817"
   },
   "outputs": [],
   "source": [
    "agg_clus.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(agg_clus.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "silhouette_score(df, agg_clus.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did not accomplish anything meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit(df)\n",
    "df_tfidf = tfidf.transform(df)\n",
    "\n",
    "df_tfidf = pd.DataFrame(columns=tfidf.get_feature_names(), data=df)\n",
    "display(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this vectorization is equivalent to the bag of words. Not necessary, and does not improve anything."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
