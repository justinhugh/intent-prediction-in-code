{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load up some libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os.path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# We will use tensorflow.tensorflow.keras in this notebook\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate dataframes to import.\n",
    "list_dfs = ['pickled_conala_mined_df', 'pickled_conala_train_df', 'pickled_conala_test_df',\n",
    "           'conala_train_bag_df', 'conala_mined_bag_df', 'combined_bag_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Load all data in list_dfs\n",
    "data = {}\n",
    "for df in list_dfs:\n",
    "    dbfile = open(df, 'rb')      \n",
    "    contents = pickle.load(dbfile)\n",
    "    data[df] = contents\n",
    "    dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pickled_conala_mined_df', 'pickled_conala_train_df', 'pickled_conala_test_df', 'conala_train_bag_df', 'conala_mined_bag_df', 'combined_bag_df'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Conala Training Data DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data['pickled_conala_train_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create series of all code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         sum(d * 10 ** i for i, d in enumerate(x[::-1]))\n",
       "1                           r = int(''.join(map(str, x)))\n",
       "2       datetime.strptime('2010-11-13 10:33:54.227806'...\n",
       "3       [(i, sum(j) / len(j)) for i, j in list(d.items...\n",
       "4                                     zip([1, 2], [3, 4])\n",
       "                              ...                        \n",
       "2374                            \"\"\"\"\"\".join([1, 2, 3, 4])\n",
       "2375    line = line.decode('utf-8', 'ignore').encode('...\n",
       "2376                                   os.system(command)\n",
       "2377    c.execute('SELECT * FROM foo WHERE bar = %s AN...\n",
       "2378    dateobj = datetime.datetime.strptime(datestr, ...\n",
       "Name: snippet, Length: 2379, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_snippets = train_df['snippet']\n",
    "train_snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a dataset of all the code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Akable0ceKeV"
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "unique_chars = []\n",
    "# cycle through all the snippets\n",
    "\n",
    "for snippet in train_snippets:\n",
    "    \n",
    "    # split the snippet into its individual characters\n",
    "    snippet_characters = list(snippet.lower())\n",
    "    dataset.append(snippet_characters)\n",
    "    \n",
    "    # List of all unique characters\n",
    "    for char in snippet_characters:\n",
    "        if char not in unique_chars:\n",
    "            unique_chars.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s',\n",
       " 'u',\n",
       " 'm',\n",
       " '(',\n",
       " 'd',\n",
       " ' ',\n",
       " '*',\n",
       " '1',\n",
       " '0',\n",
       " 'i',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ',',\n",
       " 'n',\n",
       " 'e',\n",
       " 'a',\n",
       " 't',\n",
       " 'x',\n",
       " '[',\n",
       " ':',\n",
       " '-',\n",
       " ']',\n",
       " ')',\n",
       " '=',\n",
       " \"'\",\n",
       " '.',\n",
       " 'j',\n",
       " 'p',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " '4',\n",
       " '7',\n",
       " '8',\n",
       " '6',\n",
       " '%',\n",
       " 'y',\n",
       " 'h',\n",
       " '/',\n",
       " 'l',\n",
       " 'z',\n",
       " '{',\n",
       " '}',\n",
       " 'b',\n",
       " '?',\n",
       " '<',\n",
       " '!',\n",
       " '\\\\',\n",
       " '+',\n",
       " 'v',\n",
       " '_',\n",
       " '\"',\n",
       " '@',\n",
       " 'w',\n",
       " 'g',\n",
       " '^',\n",
       " '|',\n",
       " 'c',\n",
       " 'k',\n",
       " '9',\n",
       " 'q',\n",
       " '>',\n",
       " '#',\n",
       " '~',\n",
       " ';',\n",
       " '$',\n",
       " '\\n',\n",
       " '&',\n",
       " '\\x01',\n",
       " '`',\n",
       " 'あ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the list of characters: \n",
    "unique_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to construct dataset from loop. Use `collections.deque` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87E1q2_ueKed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2379/2379 [00:00<00:00, 7353.42it/s] \n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "# X will be the current 5 characters\n",
    "X = []\n",
    "# y will be the upcoming char.\n",
    "y = []\n",
    "\n",
    "# for each snippet\n",
    "for snippet in tqdm(dataset):\n",
    "    char_deque = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    \n",
    "    # loop through characters and place them in a deque\n",
    "    # the oldest character will be thrown out each iter\n",
    "    for i in range(len(snippet)-1):\n",
    "        char = snippet[i]\n",
    "        char_deque.append(char)\n",
    "        \n",
    "        if (len(char_deque) == SEQUENCE_LENGTH):\n",
    "            X.append(list(char_deque))\n",
    "            y.append(snippet[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['s', 'u', 'm', '(', 'd', ' ', '*', ' ', '1', '0']\n",
      "y:  \n",
      "----\n",
      "X: ['u', 'm', '(', 'd', ' ', '*', ' ', '1', '0', ' ']\n",
      "y: *\n",
      "----\n",
      "X: ['m', '(', 'd', ' ', '*', ' ', '1', '0', ' ', '*']\n",
      "y: *\n",
      "----\n",
      "X: ['(', 'd', ' ', '*', ' ', '1', '0', ' ', '*', '*']\n",
      "y:  \n",
      "----\n",
      "X: ['d', ' ', '*', ' ', '1', '0', ' ', '*', '*', ' ']\n",
      "y: i\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Inspect X an y\n",
    "for i in range(5):\n",
    "    print(\"X:\",X[i])\n",
    "    print(\"y:\",y[i])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still need to convert the arrays in to np.array for RNN to consume. \n",
    "Still need to convert into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itTazOureKei"
   },
   "outputs": [],
   "source": [
    "number_to_char = {i: j for i,j in enumerate(unique_chars)}\n",
    "char_to_number = {j: i for i,j in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these dics, convert every char in X and y into a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMWxJkLGeKel"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    for j in range(len(X[0])):\n",
    "        X[i][j] = char_to_number[X[i][j]]\n",
    "        \n",
    "    y[i] = char_to_number[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "wjWoZ2qqeKem",
    "outputId": "dfb05860-f99f-49c7-feb6-26351cd5e6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [0, 1, 2, 3, 4, 5, 6, 5, 7, 8]\n",
      "y: 5\n",
      "---\n",
      "X: [1, 2, 3, 4, 5, 6, 5, 7, 8, 5]\n",
      "y: 6\n",
      "---\n",
      "X: [2, 3, 4, 5, 6, 5, 7, 8, 5, 6]\n",
      "y: 6\n",
      "---\n",
      "X: [3, 4, 5, 6, 5, 7, 8, 5, 6, 6]\n",
      "y: 5\n",
      "---\n",
      "X: [4, 5, 6, 5, 7, 8, 5, 6, 6, 5]\n",
      "y: 9\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"X:\",X[i])\n",
    "    print(\"y:\",y[i])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make in to numpy arrays, and remember the format needs to be n x q x d. \n",
    "reshape is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70949, 10, 1)\n",
      "y shape: (70949,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        ...,\n",
       "        [ 5],\n",
       "        [ 7],\n",
       "        [ 8]],\n",
       "\n",
       "       [[ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        ...,\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 5]],\n",
       "\n",
       "       [[ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        ...,\n",
       "        [ 8],\n",
       "        [ 5],\n",
       "        [ 6]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2],\n",
       "        [21],\n",
       "        [36],\n",
       "        ...,\n",
       "        [ 4],\n",
       "        [16],\n",
       "        [17]],\n",
       "\n",
       "       [[21],\n",
       "        [36],\n",
       "        [ 4],\n",
       "        ...,\n",
       "        [16],\n",
       "        [17],\n",
       "        [15]],\n",
       "\n",
       "       [[36],\n",
       "        [ 4],\n",
       "        [25],\n",
       "        ...,\n",
       "        [17],\n",
       "        [15],\n",
       "        [ 3]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "y = np.array(y)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to shuffle data so as to remove bias from the order of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zj_3W33eKey"
   },
   "outputs": [],
   "source": [
    "# Function to shuffle data.\n",
    "def shuffle_data(X_data, y_data):\n",
    "    \n",
    "    y_data = y_data.reshape((y_data.shape[0], 1, 1))\n",
    "    combined_data = np.hstack((X_data, y_data))\n",
    "    \n",
    "    np.random.shuffle(combined_data)\n",
    "\n",
    "    X_data = combined_data[:, :-1]\n",
    "    y_data = combined_data[:, -1]\n",
    "    \n",
    "    return X_data, y_data.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orQkPef8eKe0"
   },
   "outputs": [],
   "source": [
    "X, y = shuffle_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XaLzbf29nggx",
    "outputId": "031911b3-c5e8-482f-869a-965651f5dfec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70949, 10, 1)\n",
      "(70949, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling the Test Data for Later\n",
    "We have test data, so validation data is not necessary. But we'll have to transform the test data as we have done for the Train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   os.kill(os.getpid(), signal.SIGUSR1)\n",
       "1                bytes.fromhex('4a4b4c').decode('utf-8')\n",
       "2                    all(x == myList[0] for x in myList)\n",
       "3      print('%*s : %*s' % (20, 'Python', 20, 'Very G...\n",
       "4                      d.decode('cp1251').encode('utf8')\n",
       "                             ...                        \n",
       "495     re.findall('http://[^t][^s\"]+\\\\.html', document)\n",
       "496              mystring.replace(' ', '! !').split('!')\n",
       "497                                      open(path, 'r')\n",
       "498    [[sum(item) for item in zip(*items)] for items...\n",
       "499                                   a[:, (np.newaxis)]\n",
       "Name: snippet, Length: 500, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = data['pickled_conala_test_df']\n",
    "test_snippets = test_df['snippet']\n",
    "test_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XyMtmWvJBKZd",
    "outputId": "32088914-a877-420a-8a97-40dcd752eba6"
   },
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "test_unique_chars = []\n",
    "\n",
    "for snippet in test_snippets:\n",
    "    \n",
    "    # split the snippet into its individual characters\n",
    "    snippet_characters = list(snippet.lower())\n",
    "    test_dataset.append(snippet_characters)\n",
    "    \n",
    "    # List of all unique characters\n",
    "    for char in snippet_characters:\n",
    "        if char not in test_unique_chars:\n",
    "            test_unique_chars.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all unique chars in test set are in train set.\n",
    "for i in test_unique_chars:\n",
    "    if i not in unique_chars:\n",
    "        print(\"problem!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87E1q2_ueKed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 8055.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 0, 26, 59, 9, 40, 40, 3, 11, 0], [0, 26, 59, 9, 40, 40, 3, 11, 0, 26], [26, 59, 9, 40, 40, 3, 11, 0, 26, 55], [59, 9, 40, 40, 3, 11, 0, 26, 55, 15], [9, 40, 40, 3, 11, 0, 26, 55, 15, 17], [40, 40, 3, 11, 0, 26, 55, 15, 17, 28], [40, 3, 11, 0, 26, 55, 15, 17, 28, 9], [3, 11, 0, 26, 55, 15, 17, 28, 9, 4], [11, 0, 26, 55, 15, 17, 28, 9, 4, 3], [0, 26, 55, 15, 17, 28, 9, 4, 3, 23]] \n",
      "\n",
      "[26, 55, 15, 17, 28, 9, 4, 3, 23, 13]\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "# X will be the current 5 characters\n",
    "X_test = []\n",
    "# y will be the upcoming char.\n",
    "y_test = []\n",
    "\n",
    "# for each snippet\n",
    "for snippet in tqdm(test_dataset):\n",
    "    char_deque = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    \n",
    "    # loop through characters and place them in a deque\n",
    "    # the oldest character will be thrown out each iter\n",
    "    for i in range(len(snippet)-1):\n",
    "        char = snippet[i]\n",
    "        char_deque.append(char)\n",
    "        \n",
    "        if (len(char_deque) == SEQUENCE_LENGTH):\n",
    "            X_test.append(list(char_deque))\n",
    "            y_test.append(snippet[i+1])\n",
    "\n",
    "# Change from chars to numbers with dictionaries\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[0])):\n",
    "        X_test[i][j] = char_to_number[X_test[i][j]]\n",
    "        \n",
    "    y_test[i] = char_to_number[y_test[i]]\n",
    "\n",
    "# Print samples of X_test and y_test\n",
    "print(X_test[:10], \"\\n\")\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (16589, 10, 1)\n",
      "y_test shape: (16589,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[11],\n",
       "        [ 0],\n",
       "        [26],\n",
       "        ...,\n",
       "        [ 3],\n",
       "        [11],\n",
       "        [ 0]],\n",
       "\n",
       "       [[ 0],\n",
       "        [26],\n",
       "        [59],\n",
       "        ...,\n",
       "        [11],\n",
       "        [ 0],\n",
       "        [26]],\n",
       "\n",
       "       [[26],\n",
       "        [59],\n",
       "        [ 9],\n",
       "        ...,\n",
       "        [ 0],\n",
       "        [26],\n",
       "        [55]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3],\n",
       "        [14],\n",
       "        [28],\n",
       "        ...,\n",
       "        [16],\n",
       "        [18],\n",
       "        [ 9]],\n",
       "\n",
       "       [[14],\n",
       "        [28],\n",
       "        [26],\n",
       "        ...,\n",
       "        [18],\n",
       "        [ 9],\n",
       "        [ 0]],\n",
       "\n",
       "       [[28],\n",
       "        [26],\n",
       "        [14],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 0],\n",
       "        [23]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create arrays, and reshape\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "y_test = np.array(y_test)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "Try a regular neural network first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-ZZuHkoBKZU"
   },
   "outputs": [],
   "source": [
    "# Create training and validation sets\n",
    "\n",
    "# train_test_split can also work here\n",
    "validate_set_size = int(0.1 * X.shape[0])\n",
    "\n",
    "train_set_limit = X.shape[0] - validate_set_size\n",
    "\n",
    "# Split train\n",
    "train_X = X[:train_set_limit]\n",
    "train_y = y[:train_set_limit]\n",
    "\n",
    "# Split validation\n",
    "validation_X = X[train_set_limit : ]\n",
    "validation_y = y[train_set_limit : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UCzHepoJBKZZ",
    "outputId": "4013056c-ff81-4702-e4e4-e16e11c22392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63855, 10, 1)\n",
      "(7094, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)      \n",
    "print(validation_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9O2DqJ8BKZc"
   },
   "source": [
    "Need to flatten each data points from a 2D tensor to 1D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XyMtmWvJBKZd",
    "outputId": "32088914-a877-420a-8a97-40dcd752eba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63855, 10)\n",
      "(7094, 10)\n"
     ]
    }
   ],
   "source": [
    "flat_train_X = np.reshape(train_X, (-1, train_X.shape[1]))\n",
    "flat_validation_X = np.reshape(validation_X, (-1, validation_X.shape[1]))\n",
    "\n",
    "print(flat_train_X.shape)\n",
    "print(flat_validation_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "model1 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1024)              11264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 612)               627300    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 612)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 612)               2448      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                19616     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 72)                2376      \n",
      "=================================================================\n",
      "Total params: 667,100\n",
      "Trainable params: 663,828\n",
      "Non-trainable params: 3,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# First layer.\n",
    "model1.add(Dense(1024, activation='relu', input_shape=(flat_train_X.shape[1:])))\n",
    "model1.add(Dropout(0.1)) # randomly drop 10% of the previous layer output\n",
    "model1.add(BatchNormalization())\n",
    "# Second layer\n",
    "model1.add(Dense(612, activation='relu'))\n",
    "model1.add(Dropout(0.1))\n",
    "model1.add(BatchNormalization())\n",
    "# Third\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dropout(0.1))\n",
    "# Final layer using softmax\n",
    "class_number = len(unique_chars) # Number of outputs as the unique chars we have.\n",
    "model1.add(Dense(class_number, activation='softmax'))\n",
    "\n",
    "#Optimizer\n",
    "sgd = SGD(lr=0.01, decay=0.0, momentum=0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "# Compile model. \n",
    "model1.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Show model summary\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights model from disk\n",
      "No need to train, model is fully trained\n",
      "34/34 [==============================] - 1s 21ms/step - loss: 3.0429 - accuracy: 0.2509 0s - loss: 3.0463 - accuracy: 0.\n",
      "Accuracy score: 25.09%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40   # number of times through all data\n",
    "BATCH_SIZE = int(flat_train_X.shape[0]/300)   # 300 batches per epoch\n",
    "# Train if model is not yet existing\n",
    "if not os.path.exists('models/model1.h5'):\n",
    "\n",
    "    model1.fit(\n",
    "        flat_train_X, train_y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(flat_validation_X, validation_y),\n",
    "        verbose=1)\n",
    "\n",
    "    model_json = model1.to_json()\n",
    "    with open(\"models/model1.json\", \"w+\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model1.save_weights(\"models/model1.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "# If model is already saved, use it.    \n",
    "else:\n",
    "    model1.load_weights(\"models/model1.h5\")\n",
    "    print(\"Loaded weights model from disk\") \n",
    "    print(\"No need to train, model is fully trained\")\n",
    "    loss_score, accuracy_score = model1.evaluate(flat_validation_X, validation_y, batch_size=BATCH_SIZE, verbose=1)\n",
    "    print(\"Accuracy score: \"+str(round(accuracy_score*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ACu_AKzEoOlY",
    "outputId": "33a48cec-07e0-484b-956e-81414f3aaf10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print('hello world')bk.ri2'rt=pa))f'],)3ya)0o',m'ertl' l]db%a se_egotrt(s.))[ron_ta(eb'){{a)),l)]t('l_p)=]5*'ve8sc -.d'%'.')h.l/3]gn d(')pd+s+ .=e:gs\\%3}(a/\\\\'plst\\le)(?]kae ri(a(en zere] e.a s,ila re >3y=p)mc.mr]rdtlou[\n"
     ]
    }
   ],
   "source": [
    "input_phrase = \"print('hello world')\"\n",
    "\n",
    "# we will predict 200 characters forward after the input_phrase\n",
    "for i in range(200):\n",
    "    \n",
    "    # get the last 10 characters of our input_phrase and convert them to numbers\n",
    "    network_input = list(input_phrase[-SEQUENCE_LENGTH:])\n",
    "    for j in range(len(network_input)):\n",
    "        network_input[j] = char_to_number[network_input[j]]\n",
    "    # convert into an array then reshape it to explicitly have 1 feature\n",
    "    network_input = np.array(network_input)\n",
    "    network_input = network_input.reshape((1, SEQUENCE_LENGTH))\n",
    "\n",
    "    # get probabilistic predictions from the neural network\n",
    "    # randomly draw a single predicted character from the full list with their probabilities \n",
    "    # determined by the network's prediction\n",
    "    predict_proba = model1.predict(network_input)[0]\n",
    "    predict_char = np.random.choice(unique_chars, 1, p = predict_proba)[0]\n",
    "\n",
    "    input_phrase += predict_char\n",
    "\n",
    "print(input_phrase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, but the structure of this text does actually seem code-like! That's a good sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gAJIeohypR6u",
    "outputId": "86a69376-f0bb-49e4-f147-0327b33b2116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63855, 10, 1)\n",
      "(7094, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(validation_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 1024)          4202496   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 612)               4007376   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 612)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 612)               2448      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                19616     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 72)                2376      \n",
      "=================================================================\n",
      "Total params: 8,238,408\n",
      "Trainable params: 8,235,136\n",
      "Non-trainable params: 3,272\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model2.add(LSTM(1024, activation='relu', input_shape=(train_X.shape[1:]), return_sequences=True))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(BatchNormalization())\n",
    "# Layer 2\n",
    "model2.add(LSTM(612, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(BatchNormalization())\n",
    "# Layer 3\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "# Output\n",
    "class_number = len(unique_chars)\n",
    "model2.add(Dense(class_number, activation='softmax'))\n",
    "\n",
    "#Optimizer\n",
    "sgd = SGD(lr=0.01, decay=0.0, momentum=0.0, nesterov=False, clipnorm=2.0)\n",
    "\n",
    "# Compile model\n",
    "model2.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display its summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "_CmvSWSABKaA",
    "outputId": "e7b3d995-aeca-429a-9ee7-2a7f97941a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from h5 file\n",
      "Loaded weights from disk\n",
      "No need to train, model is fully trained. Validation might take a while...\n",
      "34/34 [==============================] - 39s 1s/step - loss: 2.6624 - accuracy: 0.3345\n",
      "Accuracy score: 33.45%\n"
     ]
    }
   ],
   "source": [
    "# Train if model is not yet existing\n",
    "EPOCHS = 40   # number of times through all data\n",
    "BATCH_SIZE = int(flat_train_X.shape[0]/300)   # 300 batches per epoch\n",
    "\n",
    "if not os.path.exists('models/model2.h5'):\n",
    "\n",
    "\n",
    "    model2.fit(train_X, train_y,\n",
    "                   batch_size=BATCH_SIZE,\n",
    "                   epochs=EPOCHS,\n",
    "                   validation_data=(validation_X, validation_y))\n",
    "    \n",
    "    model_json = model2.to_json()\n",
    "    with open(\"models/model2.json\", \"w+\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model2.save_weights(\"models/model2.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "else:\n",
    "    # load weights into new model\n",
    "    print(\"Loading weights from h5 file\")\n",
    "    model2.load_weights(\"models/model2.h5\")\n",
    "    print(\"Loaded weights from disk\")\n",
    "    print(\"No need to train, model is fully trained. Validation might take a while...\")\n",
    "    loss_score, accuracy_score = model2.evaluate(validation_X, validation_y, int(flat_train_X.shape[0]/300), verbose=1)\n",
    "    print(\"Accuracy score: \"+str(round(accuracy_score*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DzKM0HKOBKaC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import matplotlib.pyplot as plt(1i[)[, _eur,tst\n",
      "eicenx', 2kurr'splyilgg=), re-(cey=lmb:neelonl'oru=i().&    cas\n"
     ]
    }
   ],
   "source": [
    "input_phrase = \"import matplotlib.pyplot as plt\"\n",
    "\n",
    "# we will predict 200 characters forward after the input_phrase\n",
    "for i in range(80):\n",
    "    \n",
    "    # get the last 10 characters of our input_phrase and convert them to numbers\n",
    "    network_input = list(input_phrase[-SEQUENCE_LENGTH:])\n",
    "    for j in range(len(network_input)):\n",
    "        network_input[j] = char_to_number[network_input[j]]\n",
    "    # convert into an array then reshape it to explicitly have 1 feature\n",
    "    network_input = np.array(network_input, dtype=np.float32)\n",
    "    network_input = network_input.reshape((1, SEQUENCE_LENGTH, 1))\n",
    "\n",
    "    # get probabilistic predictions from the neural network\n",
    "    # randomly draw a single predicted character from the full list with their probabilities \n",
    "    # determined by the network's prediction\n",
    "    predict_proba = model2.predict(network_input)[0]\n",
    "    predict_char = np.random.choice(unique_chars, 1, p = predict_proba)[0]\n",
    "    \n",
    "    input_phrase += predict_char\n",
    "    print(i, end=\"\\r\")\n",
    "    \n",
    "print(input_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DzKM0HKOBKaC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network_input = ') for x in list(d.items())) for i in list(d.items())) for i in list(d.items())\n"
     ]
    }
   ],
   "source": [
    "# Modify to use argmax\n",
    "input_phrase = \"network_input =\"\n",
    "\n",
    "# we will predict 200 characters forward after the input_phrase\n",
    "for i in range(80):\n",
    "    \n",
    "    # get the last 10 characters of our input_phrase and convert them to numbers\n",
    "    network_input = list(input_phrase[-SEQUENCE_LENGTH:])\n",
    "    for j in range(len(network_input)):\n",
    "        network_input[j] = char_to_number[network_input[j]]\n",
    "    # convert into an array then reshape it to explicitly have 1 feature\n",
    "    network_input = np.array(network_input, dtype=np.float32)\n",
    "    network_input = network_input.reshape((1, SEQUENCE_LENGTH, 1))\n",
    "\n",
    "    # get probabilistic predictions from the neural network\n",
    "    # randomly draw a single predicted character from the full list with their probabilities \n",
    "    # determined by the network's prediction\n",
    "    # predict_proba = model2.predict(network_input)[0]\n",
    "    pred = np.argmax(model2.predict(network_input)[0])\n",
    "    predict_char = unique_chars[pred]\n",
    "    \n",
    "    input_phrase += predict_char\n",
    "    print(i, end=\"\\r\")\n",
    "    \n",
    "print(input_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Word2Vec, we need a list of all the sentences which will be transformed in it. So this will have to be done for both intent, and snippet. We can assemble this by combining the `conala_train_df` and the `conala_mined_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_train_df = data[\"pickled_conala_train_df\"]\n",
    "conala_mined_df = data[\"pickled_conala_mined_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two dfs.\n",
    "df = pd.concat([conala_train_df, conala_mined_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the text in intent field. (Note this is NOT using the \n",
    "# rewritten intent in the training data.)\n",
    "intent_text = list(df[\"intent\"])\n",
    "\n",
    "# Create a list of the code snippets in the data. \n",
    "snippet_text = list(df[\"snippet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_corpus = conala_train_df[\"rewritten_intent\"].str.cat(sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Cleaning the text\n",
    "processed_intent = intent_corpus.lower()\n",
    "processed_intent = re.sub('[^a-zA-Z]', ' ', processed_intent)\n",
    "processed_intent = re.sub(r'\\s+', ' ', processed_intent)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_intent)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, min_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['without']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words = word2vec.wv.most_similar('without')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.vocab\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "print(intent_text[:10])\n",
    "print(snippet_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get each unique word in the text, and for the code, each unique char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique words in text\n",
    "intent_tokens = set()\n",
    "    \n",
    "for intent in tqdm(intent_text):\n",
    "    for word in intent.split(\" \"):\n",
    "        intent_tokens.add(word)\n",
    "\n",
    "num_intent_tokens = len(intent_tokens)\n",
    "intent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(intent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_intent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data with N-grams\n",
    "from nltk import ngrams\n",
    "import itertools\n",
    "\n",
    "gram_size = 4\n",
    "data = []\n",
    "\n",
    "# Go over each intent statement\n",
    "for intent in tqdm(intent_text):\n",
    "    # Finds all n-grams in the statement\n",
    "    grams = ngrams(intent.split(), gram_size)\n",
    "    for gram in grams:\n",
    "        # Find all pairs of words within this n-gram\n",
    "        for pair in itertools.permutations(gram, 2):\n",
    "            data.append(pair)\n",
    "\n",
    "data[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = encoder.fit(list(intent_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(one_hot_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the input/output pairs:\n",
    "intent_train_data = []\n",
    "intent_train_target = []\n",
    "\n",
    "for pair in tqdm(data[:1000]):\n",
    "    intent_train_data.append(one_hot_encoder.transform([pair[0]]))\n",
    "    intent_train_target.append(one_hot_encoder.transform([pair[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_train_data = np.squeeze(np.asarray(intent_train_data))\n",
    "intent_train_target = np.squeeze(np.asarray(intent_train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Pickle the data for use later, avoiding lengthy one-hot encoding again. \n",
    "# intent_train_data\n",
    "with open('pickled_intent_train_data.pkl', 'wb+') as f:\n",
    "    # source, destination \n",
    "    pickle.dump(intent_train_data, f)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "processed_intent_text = intent_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(intent_train_data, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent_train_target\n",
    "with open('pickled_intent_train_target.pkl', 'wb+') as f:\n",
    "    # source, destination \n",
    "    pickle.dump(intent_train_target, f)                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(keras.layers.Dense(num_intent_tokens, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.CategoricalCrossentropy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "# Printout a single verbose fit operation 10 times throughout the training process.\n",
    "for i in range(0, 10):\n",
    "    model.fit(intent_train_data, intent_train_target, epochs=round(num_epochs/10)-1, verbose=0)\n",
    "    \n",
    "    print(f\"Epoch: {(i+1)*round(num_epochs/10)}/{num_epochs}\")\n",
    "    model.fit(intent_train_data, intent_train_target, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
