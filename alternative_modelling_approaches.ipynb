{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Modelling Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ 5.2 - Vectorizing Text Data](#Vectorizing-Text-Data) && Should this go before combining?\n",
    "    - [ 5.3.1 - Simple Bag of Words Vectorization](#Simple-Bag-of-Words-Vectorization)\n",
    "        - [ 5.3.1.1 - Vectorizing `conala_train_df` with Bag of Words](#Vectorizing-conala_train_df-with-Bag-of-Words)\n",
    "        - [ 5.3.1.2 - Vectorizing `conala_mined_df` with Bag of Words](#Vectorizing-conala_mined_df-with-Bag-of-Words)\n",
    "        - [ 5.3.1.3 - Comparing Vectorized `conala_mined_df` and `conala_trained_df`](#Comparing-Vectorized-conala_mined_df-and-conala_trained_df)\n",
    "        - [ 5.3.1.4 - Combining DataFrames](#Combining-DataFrames)\n",
    "        - [ 5.3.1.5 - Dimension Reduction of Bag of Words](#Dimension-Reduction-of-Bag-of-Words)\n",
    "            - [ 5.3.1.5.1 - PCA on Bag of Words](#PCA-on-Bag-of-Words)\n",
    "            - [ 5.3.1.5.2 - T-SNE on Bag of Words](#T-SNE-on-Bag-of-Words)\n",
    "    - [ 5.3.2 - Word2Vec Text Vectorization](#Word2Vec-Text-Vectorization)\n",
    "        - [ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent Paradigms\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "We can look at the above graph to see some common themes which emerge, at least on the level of word frequency. \n",
    "\n",
    "- String manipulation \n",
    "- List manipulation \n",
    "- Type change\n",
    "- Regular Expression\n",
    "- DataFrame Manipulation\n",
    "- Find object  \n",
    "\n",
    "\n",
    "&&...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Bag of Words Vectorization\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing `conala_train_df` with Bag of Words\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intent               0\n",
       "rewritten_intent    79\n",
       "snippet              0\n",
       "question_id          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for nan\n",
    "conala_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with \"\"\n",
    "conala_train_df.fillna('', inplace=True)\n",
    "\n",
    "conala_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate \n",
    "conala_train_bagofwords = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "\n",
    "# Fit \n",
    "conala_train_bagofwords.fit(conala_train_df[\"rewritten_intent\"])\n",
    "\n",
    "# Transform with the bag of words.\n",
    "conala_train_bag_SM = conala_train_bagofwords.transform(conala_train_df[\"rewritten_intent\"])\n",
    "conala_train_bag_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame (more workable) from the Sparse Matrix \n",
    "conala_train_bag_df = pd.DataFrame(columns=conala_train_bagofwords.get_feature_names(),\n",
    "                                   data=conala_train_bag_SM.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_train_bag_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing `conala_test_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nan\n",
    "conala_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with \"\"\n",
    "conala_test_df.fillna('', inplace=True)\n",
    "\n",
    "conala_test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform with the bag of words from the train df\n",
    "conala_test_bag_SM = conala_train_bagofwords.transform(conala_test_df[\"rewritten_intent\"])\n",
    "conala_test_bag_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame (more workable) from the Sparse Matrix \n",
    "conala_test_bag_df = pd.DataFrame(columns=conala_train_bagofwords.get_feature_names(),\n",
    "                                   data=conala_test_bag_SM.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is our test set, we shouldn't peek at the results of the transformation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Reduction of Bag of Words\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA on Bag of Words\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### T-SNE on Bag of Words\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Text Vectorization\n",
    "[[Back To TOC]](#Table-of-Contents)\n",
    "\n",
    "Word2Vec Embeddings are \n",
    "\n",
    "See also Doc2Vec, FastText and wrappers for VarEmbed and WordRank.\n",
    "[[x]](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gensim, and get word2vec model methods. \n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader # allows downloading of existing models\n",
    "\n",
    "# Downloading a pre-trained vector using 50 dimensions, from twitter data\n",
    "wv = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking vocab type\n",
    "type(wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terms in vocab\n",
    "len(wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for similar terms, cosine similarity!\n",
    "wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if word is in wv vocab\n",
    "\"cat\" in wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique word are in our corpus?\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now check how many of these are in the word2vec pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the list of words contained in model, and those missing.\n",
    "contained=[] # list of terms in both our corpus and the model\n",
    "missing=[] # list of terms in our corpus, but not the model\n",
    "msk=[] # True/false mask for unique words that are in the model. \n",
    "for i in unique_words:\n",
    "    if(i in wv.vocab):\n",
    "        msk.append(1)\n",
    "        contained.append(i)\n",
    "    else:\n",
    "        msk.append(0)\n",
    "        missing.append(i)\n",
    "sum(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at missing words\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&&&& Loading Pre-existing vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&&&&& When using Word2Vec, there's much extra thought to be given regarding how the sentences I'm feeding to the model will be handled. There's a large number of special characters such as brackets and \"%\" for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&&&&& Comparing the unique words to vocab of pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple of functions to help process lists of text sentences.\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_split_text_list(li):\n",
    "    '''\n",
    "    Takes a list of sentences.\n",
    "    Returns a list of lists, each inner list is words in a sentence.\n",
    "    Also adds a space on either side of non-word, non-digit chars. \n",
    "    This allows for brackets, etc. to be considered as their own word, unless \n",
    "    vectorized with a model which does not include them.\n",
    "    '''\n",
    "    \n",
    "    new_list = list()\n",
    "    for i in li:\n",
    "        try:\n",
    "            i = i.lower() #lowercase the sentence\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            i = re.sub('([^a-zA-Z\\ \\d])', r' \\1 ', i) # Add spaces between special chars\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            i = list(i.split(' '))\n",
    "        except:\n",
    "            pass\n",
    "        new_list.append(i)\n",
    "    return new_list\n",
    "\n",
    "def vectorize_text_list(li):\n",
    "    '''\n",
    "    Takes a list of lists.\n",
    "        - first list is a sentence\n",
    "        - inner list is a list of words.\n",
    "    Returns a list of lists, each inner list is words in a sentence.\n",
    "    Also adds a space on either side of non-word, non-digit chars. \n",
    "    This allows for brackets, etc. to be considered as their own word, unless \n",
    "    vectorized with a model which does not include them.\n",
    "    '''\n",
    "    new_list=list() # new list object to be returned at end.\n",
    "    for i in li:\n",
    "        if i == None:\n",
    "            new_list.append(np.zeros_like(wv[\"empty\"])) # If None, empty array of wv shape.\n",
    "            continue\n",
    "        if type(i) == float:\n",
    "            i = str(i)\n",
    "        sub_list=list() # list of vecs, representing a sentence\n",
    "        for j in i: \n",
    "            try:\n",
    "                vec = wv[j]\n",
    "                sub_list.append(vec)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        new_list.append(sub_list)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on Word2Vec\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-SNE on Word2Vec\n",
    "[[Back To TOC]](#Table-of-Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
